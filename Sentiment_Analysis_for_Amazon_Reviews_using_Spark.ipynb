{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis for Amazon Reviews using Spark",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2SbZFlCaUiUiEQz6J8Rpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yulucy19/Thinkful-Project-2019/blob/master/Sentiment_Analysis_for_Amazon_Reviews_using_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dogaI-OQ4CgO",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Analysis for Amazon Reviews using Spark**\n",
        "\n",
        "In this project, we'll work on a sentiment analysis dataset: the Amazon reviews dataset. \n",
        "\n",
        "It's always important to start with a clear goal in mind. In this case, we'd like to determine if we can predict whether a review is positive or negative based on the language in the review.\n",
        "\n",
        "We're going to tackle this problem with Spark, and some tips to help us get started:\n",
        "\n",
        "* Pyspark always needs to point at a running Spark instance. You can do that using a SparkContext.\n",
        "* We're working in batch mode, so you'll need to load an entire file into memory in order to run any models you build.\n",
        "* Spark likes to execute models in a pipeline, so remember that when the time comes to set up your model.\n",
        "* Spark's machine learning algorithms expect numeric variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YwiTnYD6fMW",
        "colab_type": "text"
      },
      "source": [
        "## Spark and Colaboratory Setup\n",
        "\n",
        "First, there is some configration specific to running Spark on Colaboratory that we'll need to attend to. Run these cells to set everything up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8opKlhHJ2OeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Apache Spark 2.4.5 and Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4re6Cf4d6wzf",
        "colab_type": "code",
        "outputId": "66c18b1b-24b9-47a4-dbc1-2ad2d4617e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Install spark-related depdencies for Python\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 61kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 35.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=e5a9c7db1ad45c6c1e0f712c672f246b38ad79be0f4250b10aa82dd53e8b4699\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm4dfECu603C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up required environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu1_qxvO65O2",
        "colab_type": "code",
        "outputId": "7bb445b3-1d80-4cec-9749-1daf0c7570ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "# Point Colaboratory to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpAIsAZT7K-1",
        "colab_type": "text"
      },
      "source": [
        "##  Import dependencies\n",
        "\n",
        "Next, we need to import the tools we'll need from PySpark. The imports below allow us to connect to the Spark server, load our data, clean it, and prepare, execute, and evaluate a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuUgd18w6-Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import functools\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk1O_Mu_BoOY",
        "colab_type": "text"
      },
      "source": [
        "## Set our constants\n",
        "Next, we create a set of constants that we can refer to throughout the notebook. These are values that the rest of our code needs to run, but that we might need to change at some point (for instance, if the location of our data changes).\n",
        "\n",
        "The relevant datasets have been saved in my Google Drive folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_JUrljABslE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "JSON_PATH = \"/content/gdrive/My Drive/Colab Datasets/Amazon_Instant_Video_5.json\" \n",
        "#JSON_ACTIVITY_LABEL_PATH = \"/content/gdrive/My Drive/Colab Datasets/Amazon_Instant_Video_5.json\"\n",
        "APP_NAME = \"Sentiment Analysis for Amazon Instant Video Reviews\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141107\n",
        "TRAINING_DATA_RATIO = 0.8\n",
        "RF_NUM_TREES = 10\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRn2PUdlC-TO",
        "colab_type": "text"
      },
      "source": [
        "## Connect to the server and load data\n",
        "Now we're ready to connect to the Spark server. We do that (relying on the constants set above) and then load our data (loaded into `video_df`).\n",
        "\n",
        "The first thing we always do is create a SparkContext, and then immediately afterward create a sqlContext to be able to load and manipulate an RDD/dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5yawNKsC1Vm",
        "colab_type": "code",
        "outputId": "ba43a292-dabc-4cb9-9ae6-038005a2878f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "sc = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
        "# Load data directly from sc.read.options\n",
        "#video_orig_df = sc.read.options(inferschema = \"true\").json(JSON_PATH)\n",
        "\n",
        "# Let's try loading data from SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "video_orig_df = sqlContext.read.json(JSON_PATH)\n",
        "\n",
        "video_df = video_orig_df\n",
        "video_df.show(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
            "|      asin| helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
            "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
            "|B000H00VBQ|  [0, 0]|    2.0|I had big expecta...| 05 3, 2014|A11N155CW1UV02|            AdrianaM|A little bit bori...|    1399075200|\n",
            "|B000H00VBQ|  [0, 0]|    5.0|I highly recommen...| 09 3, 2012|A3BC8O2KCL29V2|             Carol T|Excellent Grown U...|    1346630400|\n",
            "|B000H00VBQ|  [0, 1]|    1.0|This one is a rea...|10 16, 2013| A60D5HQFOTSOM|Daniel Cooper \"da...|Way too boring fo...|    1381881600|\n",
            "|B000H00VBQ|  [0, 0]|    4.0|Mysteries are int...|10 30, 2013|A1RJPIGRSNX4PW|      J. Kaplan \"JJ\"|Robson Green is m...|    1383091200|\n",
            "|B000H00VBQ|  [1, 1]|    5.0|This show always ...|02 11, 2009|A16XRPF40679KG|       Michael Dobey|Robson green and ...|    1234310400|\n",
            "|B000H00VBQ|[12, 12]|    5.0|I discovered this...|10 11, 2011|A1POFVVXUZR3IQ|             Z Hayes|I purchased the s...|    1318291200|\n",
            "|B000H0X79O|  [0, 0]|    3.0|It beats watching...|10 15, 2013|A1PG2VV4W1WRPL|Jimmy C. Saunders...|It takes up your ...|    1381795200|\n",
            "|B000H0X79O|  [0, 0]|    3.0|There are many ep...|12 29, 2013| ATASGS8HZHGIB|             JohnnyC|A reasonable way ...|    1388275200|\n",
            "|B000H0X79O|  [0, 0]|    5.0|This is the best ...|02 26, 2014|A3RXD7Z44T9DHW|              Kansas|           kansas001|    1393372800|\n",
            "|B000H0X79O|  [0, 0]|    3.0|Not bad.  Didn't ...| 04 2, 2014| AUX8EUBNTHIIU| Louis V. Borsellino| Entertaining Comedy|    1396396800|\n",
            "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BzJ9ZQkO-9T",
        "colab_type": "text"
      },
      "source": [
        "## Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84BZ_jTDD2lZ",
        "colab_type": "code",
        "outputId": "af7a61f5-d5de-43f2-cf30-a428c3490a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Let's check the data set shape.\n",
        "print(\"Amazoninstantvideo dataset has {} rows and {} columns.\".format(video_df.count(), len(video_df.columns)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amazoninstantvideo dataset has 37126 rows and 9 columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMOI3-F4NGq7",
        "colab_type": "code",
        "outputId": "63e33e7b-1595-49c5-82ef-36ac87934613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "video_df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['asin',\n",
              " 'helpful',\n",
              " 'overall',\n",
              " 'reviewText',\n",
              " 'reviewTime',\n",
              " 'reviewerID',\n",
              " 'reviewerName',\n",
              " 'summary',\n",
              " 'unixReviewTime']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298OTOJ5J7i4",
        "colab_type": "text"
      },
      "source": [
        "For this sentiment analysis, from the above data table, we can see that overall column-numeric column will be our target column for performing supervised learning. And reviewText will be independent variable for the analysis. But this column is text data, we will use word2vec to convert the text review data into numerical vectors.\n",
        "\n",
        "Overall column is numerical data on a scale of 1 to 5. For sentiment analysis, we will convert this column into binary data:\n",
        "positive sentiment as 1 and negative sentiment as 0. \n",
        "\n",
        "* If overall value > 3, it is positive as 1\n",
        "* If overall value <=3, it is negative as 0.\n",
        "\n",
        "we will add another column in the dataset as overall_bin which will be our target or Y variable.\n",
        "\n",
        "Before we get going on our data preparation, let's take a look at the columns we mentioned above.\n",
        "\n",
        "To perform a SQL query on a dataframe, we need to create a `tempTable`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpiH2_qT96P1",
        "colab_type": "code",
        "outputId": "c3d44183-ce1b-4132-d49c-d0d046a6ef6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "# Check the 'overall' column statistical description.\n",
        "video_df.describe(\"overall\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|summary|           overall|\n",
            "+-------+------------------+\n",
            "|  count|             37126|\n",
            "|   mean| 4.209529709637451|\n",
            "| stddev|1.1185496668776904|\n",
            "|    min|               1.0|\n",
            "|    max|               5.0|\n",
            "+-------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5TiyGUUCh55",
        "colab_type": "code",
        "outputId": "9d7ff02a-7501-446a-aa77-0ad10bf6aa30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "# Create a TempTable\n",
        "video_df.registerTempTable('videoreviews')\n",
        "\n",
        "# Perform a SQL query to the column of \"overall\"\n",
        "sqlContext.sql(\"SELECT overall, count(overall) as reviewcount FROM videoreviews GROUP BY overall ORDER BY overall desc\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----------+\n",
            "|overall|reviewcount|\n",
            "+-------+-----------+\n",
            "|    5.0|      20890|\n",
            "|    4.0|       8446|\n",
            "|    3.0|       4187|\n",
            "|    2.0|       1885|\n",
            "|    1.0|       1718|\n",
            "+-------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VY5gwUGJKSo",
        "colab_type": "text"
      },
      "source": [
        "From the above statistical description table and reviewcount table, it can be seen thant most reviews are postive reviews about 78% (4.0+5.0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfj_koH5C77C",
        "colab_type": "code",
        "outputId": "b395711b-3db8-4808-a720-45ef53288d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "# Now let's add a new column overall_bin with number of 0 (negative sentiment) and 1(positive sentiment) from User Defined Function.\n",
        "udf = UserDefinedFunction(lambda x: 1 if x > 3.0 else 0, IntegerType())\n",
        "video_df1 = video_df.withColumn('overall_bin', udf(video_df.overall))\n",
        "video_df1.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+-----------+\n",
            "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|overall_bin|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+-----------+\n",
            "|B000H00VBQ| [0, 0]|    2.0|I had big expecta...| 05 3, 2014|A11N155CW1UV02|            AdrianaM|A little bit bori...|    1399075200|          0|\n",
            "|B000H00VBQ| [0, 0]|    5.0|I highly recommen...| 09 3, 2012|A3BC8O2KCL29V2|             Carol T|Excellent Grown U...|    1346630400|          1|\n",
            "|B000H00VBQ| [0, 1]|    1.0|This one is a rea...|10 16, 2013| A60D5HQFOTSOM|Daniel Cooper \"da...|Way too boring fo...|    1381881600|          0|\n",
            "|B000H00VBQ| [0, 0]|    4.0|Mysteries are int...|10 30, 2013|A1RJPIGRSNX4PW|      J. Kaplan \"JJ\"|Robson Green is m...|    1383091200|          1|\n",
            "|B000H00VBQ| [1, 1]|    5.0|This show always ...|02 11, 2009|A16XRPF40679KG|       Michael Dobey|Robson green and ...|    1234310400|          1|\n",
            "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xayiZK0EI-nN",
        "colab_type": "code",
        "outputId": "72a3db0e-0436-46f0-adeb-ba8ed6ec79f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "# Plot the count of positive and negative reviews\n",
        "plt.rcParams['figure.figsize'] = (15, 6)\n",
        "\n",
        "review_bin = video_df1.groupBy('overall_bin').count().collect()\n",
        "\n",
        "categories = [i[0] for i in review_bin]\n",
        "counts = [i[1] for i in review_bin]\n",
        "\n",
        "ind = np.array(range(len(categories)))\n",
        "width = 0.35\n",
        "plt.bar(ind, counts, width=width, color='b')\n",
        "\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Distribution')\n",
        "plt.xticks(ind +width/2, categories)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7fd071ff9160>,\n",
              "  <matplotlib.axis.XTick at 0x7fd071ff9128>],\n",
              " [Text(0, 0, '1'), Text(0, 0, '0')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-ZMYu2-OG7E",
        "colab_type": "text"
      },
      "source": [
        "From the above graph, it can see that the data is imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pze5gvpkOWx4",
        "colab_type": "text"
      },
      "source": [
        "Next, let's take a look at the relevant columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rMkrw3DMtVC",
        "colab_type": "code",
        "outputId": "a06c8dcf-9be5-4b05-cfda-7ac09f854faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "video_df1.select('overall', 'overall_bin','reviewText', 'summary').show(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----------+--------------------+--------------------+\n",
            "|overall|overall_bin|          reviewText|             summary|\n",
            "+-------+-----------+--------------------+--------------------+\n",
            "|    2.0|          0|I had big expecta...|A little bit bori...|\n",
            "|    5.0|          1|I highly recommen...|Excellent Grown U...|\n",
            "|    1.0|          0|This one is a rea...|Way too boring fo...|\n",
            "|    4.0|          1|Mysteries are int...|Robson Green is m...|\n",
            "|    5.0|          1|This show always ...|Robson green and ...|\n",
            "|    5.0|          1|I discovered this...|I purchased the s...|\n",
            "|    3.0|          0|It beats watching...|It takes up your ...|\n",
            "|    3.0|          0|There are many ep...|A reasonable way ...|\n",
            "|    5.0|          1|This is the best ...|           kansas001|\n",
            "|    3.0|          0|Not bad.  Didn't ...| Entertaining Comedy|\n",
            "+-------+-----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqzGIdRRPSEb",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize the review text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbeL2wPYOs7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(inputCol='reviewText', outputCol='token_text').transform(video_df1)\n",
        "\n",
        "word2Vec = Word2Vec(vectorSize=300, seed=42, inputCol='token_text', outputCol='w2v_vector').fit(tokenizer)\n",
        "\n",
        "w2vdf = word2Vec.transform(tokenizer)\n",
        "#w2vdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmFxAg_FPPsa",
        "colab_type": "code",
        "outputId": "370606f7-85b3-43de-d46e-ec459496b783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "w2vdf.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- asin: string (nullable = true)\n",
            " |-- helpful: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- overall: double (nullable = true)\n",
            " |-- reviewText: string (nullable = true)\n",
            " |-- reviewTime: string (nullable = true)\n",
            " |-- reviewerID: string (nullable = true)\n",
            " |-- reviewerName: string (nullable = true)\n",
            " |-- summary: string (nullable = true)\n",
            " |-- unixReviewTime: long (nullable = true)\n",
            " |-- overall_bin: integer (nullable = true)\n",
            " |-- token_text: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- w2v_vector: vector (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLlys0d3SifD",
        "colab_type": "text"
      },
      "source": [
        "## Build a model and run our classfier in Spark\n",
        "\n",
        "After we have everything in numeric format, we are ready to reshape the data and run the random forest model. \n",
        "\n",
        "In this project, we will use __Random Forest__, and __Logistic Regression__,  to build models for predicting sentiments. The scores from all supervised techniques were compared and then decided which technique to be selected for supervised models.\n",
        "\n",
        "In spark, we manipulate the data to work in a spark pipeline, define each of the steps in the pipeline, chain them together, and finally run the pipeline.\n",
        "\n",
        "Apache Spark classifiers expect 2 columns of input:\n",
        "1. __labels__: an indexed set of numeric variable that respect the classification from the set of features we provide.\n",
        "2. __features__: an indexed, vector variable that contains all of the feature values in each row.\n",
        "\n",
        "In order to do this, we need to create these 2 columns from our dataset - the data is there, but not yet in a format we can use for the classifier. \n",
        "\n",
        "To create the indexed label column, we'll create a column called `indexedLabel` using the `StringIndexer` method. We use the column `overall_bin` as the source for label index since that contains our labels. \n",
        "\n",
        "To create indexed features column, we'll need to use `w2v_vector` columns from our data frame. We'll call this vector `indexedFeatures`.\n",
        "\n",
        "Since the classifier expects indexed labels and an indexed vector column of data, we'll use the `indexedLabel` and `indexedFeatures` as input to our supervised learning models.\n",
        "\n",
        "Note that now all data is in w2vdf object not in the original video_df1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dlLannmUPCt",
        "colab_type": "text"
      },
      "source": [
        "Let's confirm that the features are there. It's easy to do this in Apache Spark using the select and show methods on the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xaeykdcUNyG",
        "colab_type": "code",
        "outputId": "0ccc6eff-ba3d-4432-990d-dabbe34cf867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "w2vdf.select('overall_bin','reviewText','token_text','w2v_vector').show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+--------------------+--------------------+\n",
            "|overall_bin|          reviewText|          token_text|          w2v_vector|\n",
            "+-----------+--------------------+--------------------+--------------------+\n",
            "|          0|I had big expecta...|[i, had, big, exp...|[-0.0262556820941...|\n",
            "|          1|I highly recommen...|[i, highly, recom...|[0.04453314543934...|\n",
            "|          0|This one is a rea...|[this, one, is, a...|[0.00893908824162...|\n",
            "|          1|Mysteries are int...|[mysteries, are, ...|[-0.0135645127941...|\n",
            "|          1|This show always ...|[this, show, alwa...|[0.00328893336859...|\n",
            "+-----------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIX6r38uUrqw",
        "colab_type": "text"
      },
      "source": [
        "Now we're ready to build teh indexers, split our data for training and testing, define our model, and finally chain everything together into a pipeline.\n",
        "\n",
        "It's important to note that when we execute this cell, we're not actually running our model. At this point, we're only defining its parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VElnekuPWHa",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bcpvU7YSNOa",
        "colab_type": "code",
        "outputId": "215c3ffc-e2cd-428e-9243-020e4b4f136a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Build the training indexers / split data / classifier\n",
        "# first we'll generate a labelIndexer (target variable)\n",
        "labelIndexer = StringIndexer(inputCol=\"overall_bin\", outputCol=\"indexedLabel\").fit(w2vdf)\n",
        "\n",
        "# now generate the indexed feature vector (independent variable)\n",
        "featureIndexer = VectorIndexer(inputCol=\"w2v_vector\", outputCol=\"indexedFeatures\", maxCategories=4).fit(w2vdf)\n",
        "    \n",
        "# Split the data into training and test sets (20% held out for testing)\n",
        "# set seed for reproducibility\n",
        "(training, test) = w2vdf.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO], seed=100)\n",
        "print(\"Training Dataset Count: \" + str(training.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))\n",
        "\n",
        "#Train a RandomForest model.\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
        "\n",
        "# Chain indexers and forest in a Pipeline\n",
        "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 29727\n",
            "Test Dataset Count: 7399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGba4h9VqTo",
        "colab_type": "text"
      },
      "source": [
        "This next cell runs the pipeline, delivering a trained model at the end of the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J_rQBD1VrjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model. This also runs the indexers.\n",
        "model_rf = pipeline_rf.fit(training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy-DeukSWC5e",
        "colab_type": "text"
      },
      "source": [
        "It is now easy to test our model and make predictions simply by using the model's transform method on the testData dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZUghTaeVvd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions\n",
        "predictions_rf = model_rf.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oF_OuZeWSXi",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate RF model\n",
        "\n",
        "Now we can use the MulticlassClassificationEvaluator to test the RF model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nmEPIXYWHaq",
        "colab_type": "code",
        "outputId": "7689cafb-7ff4-4d1c-dc0d-ec1a9152c39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Select (prediction, true label) and compute test error.\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions_rf)\n",
        "\n",
        "print(f\"Test Error = {(1.0-accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.211245\n",
            "Accuracy = 0.788755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDW7ZN74WjU6",
        "colab_type": "text"
      },
      "source": [
        "We didn't do so well... but that's typical in data science work.\n",
        "\n",
        "Now let's take a look at Logisitic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkiFMwSAPrZA",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8itvbOqDWZm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Build the training indexers / split data / classifier\n",
        "# first we'll generate a labelIndexer (target variable)\n",
        "labelIndexer = StringIndexer(inputCol=\"overall_bin\", outputCol=\"indexedLabel\").fit(w2vdf)\n",
        "\n",
        "# now generate the indexed feature vector (independent variable)\n",
        "featureIndexer = VectorIndexer(inputCol=\"w2v_vector\", outputCol=\"indexedFeatures\", maxCategories=4).fit(w2vdf)\n",
        "    \n",
        "# Split the data into training and test sets (20% held out for testing)\n",
        "# set seed for reproducibility\n",
        "(training, test) = w2vdf.randomSplit([0.8, 0.2],seed=100)\n",
        "#training.cache()\n",
        "\n",
        "# Logistic Regression Model\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0, labelCol=\"indexedLabel\",featuresCol=\"indexedFeatures\" )\n",
        "\n",
        "pipeline_lr = Pipeline(stages=[labelIndexer, featureIndexer, lr])\n",
        "\n",
        "# Train model. This also runs the indexers.\n",
        "model_lr = pipeline_lr.fit(training)\n",
        "\n",
        "# Make predictions\n",
        "predictions_lr = model_lr.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKASYzb2QAOC",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate LR Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK2WTnT-f4-p",
        "colab_type": "code",
        "outputId": "e6086b4f-8a88-4fa7-d147-50f386e712c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Select (prediction, true label) and compute test error.\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions_lr)\n",
        "\n",
        "print(f\"Test Error = {(1.0-accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.176645\n",
            "Accuracy = 0.823355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstU3bKh0lVo",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "The accuracy of 0.823 for logistic regression model is more than 0.789 for random forest model. Random forest is a very good, robust and versatile method, however it’s no mystery that for high-dimensional sparse data it’s not a best choice. It is obvious that Logistic Regression will be our model in this project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auPzvbLqq1AR",
        "colab_type": "text"
      },
      "source": [
        "Here's where you can go from here:\n",
        "\n",
        "1. Cross validation for logistic regression to see any improvement for the model\n",
        "2. Use a different method to tokenize and convert the text to numeric (TF/IDF, etc).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19B366wR2B5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}